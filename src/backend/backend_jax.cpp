#include "syntra/backend_jax.hpp"
#include "syntra/ast.hpp"

#include <cstring> // std::strlen
#include <sstream>
#include <string>
#include <vector>

namespace syntra {

// ---------------------------------------------------------------------------
// Small helpers to read AST literal fields
// We support both:
//   - std::vector<const NamedField*>   (like in the PyTorch backend)
//   - std::vector<NamedField>          (used by IRDataset in JAX backend)
// ---------------------------------------------------------------------------

static std::string getStringField(const std::vector<const NamedField *> &fields,
                                  const std::string &name,
                                  const std::string &defaultValue) {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (!f->value)
            continue;
        if (f->value->kind == ExprKind::StringLiteral) {
            const auto &s = static_cast<const StringLiteralExpr &>(*f->value);
            return s.value;
        }
    }
    return defaultValue;
}

static std::string getStringField(const std::vector<NamedField> &fields,
                                  const std::string &name,
                                  const std::string &defaultValue) {
    std::vector<const NamedField *> ptrs;
    ptrs.reserve(fields.size());
    for (const auto &f : fields) {
        ptrs.push_back(&f);
    }
    return getStringField(ptrs, name, defaultValue);
}

static std::string
getNumericField(const std::vector<const NamedField *> &fields,
                const std::string &name, const std::string &defaultValue) {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (!f->value)
            continue;
        if (f->value->kind == ExprKind::IntegerLiteral) {
            const auto &i = static_cast<const IntegerLiteralExpr &>(*f->value);
            return i.value;
        }
        if (f->value->kind == ExprKind::FloatLiteral) {
            const auto &fl = static_cast<const FloatLiteralExpr &>(*f->value);
            return fl.value;
        }
    }
    return defaultValue;
}

static std::string getNumericField(const std::vector<NamedField> &fields,
                                   const std::string &name,
                                   const std::string &defaultValue) {
    std::vector<const NamedField *> ptrs;
    ptrs.reserve(fields.size());
    for (const auto &f : fields) {
        ptrs.push_back(&f);
    }
    return getNumericField(ptrs, name, defaultValue);
}

static std::string getBoolField(const std::vector<const NamedField *> &fields,
                                const std::string &name,
                                const std::string &defaultValue) {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (!f->value)
            continue;
        if (f->value->kind == ExprKind::BoolLiteral) {
            const auto &b = static_cast<const BoolLiteralExpr &>(*f->value);
            return b.value ? "True" : "False";
        }
    }
    return defaultValue;
}

static std::string getBoolField(const std::vector<NamedField> &fields,
                                const std::string &name,
                                const std::string &defaultValue) {
    std::vector<const NamedField *> ptrs;
    ptrs.reserve(fields.size());
    for (const auto &f : fields) {
        ptrs.push_back(&f);
    }
    return getBoolField(ptrs, name, defaultValue);
}

// ============================================================================
// Top-level entry
// ============================================================================

std::string
JaxBackend::emitPythonScript(const PipelineModule &module,
                             const std::string &selectedExperimentName) {
    std::ostringstream out;

    out << emitHeader();
    out << "\n\n";
    out << emitDatasetSection(module);
    out << "\n\n";
    out << emitModelSection(module);
    out << "\n\n";
    out << emitExperimentSection(module);
    out << "\n\n";
    out << emitMainSection(module, selectedExperimentName);

    return out.str();
}

// ============================================================================
// Sections
// ============================================================================

std::string JaxBackend::emitHeader() const {
    std::ostringstream out;

    out << "# Auto-generated by SyntraLine++ JAX backend\n"
        << "# This script mirrors the PyTorch backend semantics where "
           "possible.\n"
        << "# It supports:\n"
        << "#   - Real MNIST CSV loading (label, p0..p783)\n"
        << "#   - CNN + MLP architectures driven by the DSL model fields\n"
        << "#   - optax-based optimizers (adam, adamw, sgd)\n"
        << "\n"
        << "from typing import Any, Dict, Tuple\n"
        << "\n"
        << "import os\n"
        << "import csv\n"
        << "import jax\n"
        << "import jax.numpy as jnp\n"
        << "import optax\n"
        << "\n"
        << "# Decide whether to try using real CSV datasets, mirroring\n"
        << "# SYNTRA_PREFER_REAL_DATASETS used by the PyTorch backend.\n"
        << "USE_REAL_DATA = os.environ.get(\"SYNTRA_PREFER_REAL_DATASETS\", "
           "\"0\").lower() in (\n"
        << "    \"1\", \"true\", \"yes\", \"on\",\n"
        << ")\n"
        << "\n"
        << "KeyArray = jax.Array\n"
        << "\n"
        << "def _log(msg: str) -> None:\n"
        << "    print(f\"[Syntra-JAX] {msg}\")\n";

    return out.str();
}

// ----------------------------------------------------------------------------
// Dataset section
// ----------------------------------------------------------------------------

std::string JaxBackend::emitDatasetSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Dataset definitions (real CSV loader + random fallback)\n"
        << "# ============================================================\n";

    if (module.datasets.empty()) {
        out << "_log(\"No datasets defined in this module.\")\n";
        return out.str();
    }

    // ---------------------------------------------------------------------
    // Random MNIST-like stub (fallback)
    // ---------------------------------------------------------------------
    // NOTE: For JAX we use NHWC layout: (N, 28, 28, 1)
    out << "def create_random_mnist_like_data(num_train: int = 1000,\n"
        << "                                   num_test: int = 200,\n"
        << "                                   seed: int = 0):\n"
        << "    \"\"\"Create fake MNIST-like data using JAX random.\n"
        << "    This is purely for structural testing.\n"
        << "    \"\"\"\n"
        << "    key = jax.random.PRNGKey(seed)\n"
        << "    key_train, key_test = jax.random.split(key)\n"
        << "\n"
        << "    x_train = jax.random.normal(key_train, (num_train, 28, 28, "
           "1))\n"
        << "    y_train = jax.random.randint(key_train, (num_train,), 0, 10)\n"
        << "    x_test  = jax.random.normal(key_test,  (num_test, 28, 28, 1))\n"
        << "    y_test  = jax.random.randint(key_test,  (num_test,), 0, 10)\n"
        << "    return (x_train, y_train), (x_test, y_test)\n"
        << "\n";

    // ---------------------------------------------------------------------
    // CSV loader for MNIST-style CSV: label, p0..p783
    // ---------------------------------------------------------------------
    out << "def _load_mnist_csv(source: str, max_rows=None):\n"
        << "    \"\"\"Load a MNIST-style CSV into (x, y) JAX arrays.\n"
        << "\n"
        << "    Each row is expected to be:\n"
        << "        label, p0, p1, ..., p783\n"
        << "    where pixels are 0-255.\n"
        << "    We return x with shape (N, 28, 28, 1), y with shape (N,).\n"
        << "    \"\"\"\n"
        << "    xs = []\n"
        << "    ys = []\n"
        << "    with open(source, \"r\") as f:\n"
        << "        reader = csv.reader(f)\n"
        << "        for i, row in enumerate(reader):\n"
        << "            if not row:\n"
        << "                continue\n"
        << "            if max_rows is not None and i >= max_rows:\n"
        << "                break\n"
        << "            label = int(row[0])\n"
        << "            pixels = [float(v) / 255.0 for v in row[1:]]\n"
        << "            if len(pixels) != 28 * 28:\n"
        << "                raise ValueError(\n"
        << "                    f\"expected 784 pixels per row, got "
           "{len(pixels)} in {source}\"\n"
        << "                )\n"
        << "            xs.append(pixels)\n"
        << "            ys.append(label)\n"
        << "    if not xs:\n"
        << "        raise ValueError(f\"file {source} was empty or invalid\")\n"
        << "    x_arr = jnp.array(xs, dtype=jnp.float32).reshape(-1, 28, 28, "
           "1)\n"
        << "    y_arr = jnp.array(ys, dtype=jnp.int32)\n"
        << "    return x_arr, y_arr\n"
        << "\n";

    // ---------------------------------------------------------------------
    // Per-dataset helpers: create_<name>_data()
    //  - If USE_REAL_DATA -> try CSV using dataset.source
    //  - Else              -> fall back to random stub
    // ---------------------------------------------------------------------
    for (const auto &d : module.datasets) {
        std::string trainSource =
            getStringField(d.fields, "source", "data/unknown.csv");

        // Derive test path: replace "_train." with "_test." if we can
        std::string testSource = trainSource;
        const char *needle = "_train.";
        auto pos = trainSource.find(needle);
        if (pos != std::string::npos) {
            testSource = trainSource;
            testSource.replace(pos, std::strlen(needle), "_test.");
        }

        out << "def create_" << d.name << "_data():\n"
            << "    \"\"\"Create train/test data for dataset '" << d.name
            << "'.\"\"\"\n"
            << "    train_source = \"" << trainSource << "\"\n"
            << "    test_source  = \"" << testSource << "\"\n"
            << "\n"
            << "    if USE_REAL_DATA:\n"
            << "        _log(\n"
            << "            f\"Using real CSV data: train={train_source}, "
               "test={test_source}\"\n"
            << "        )\n"
            << "        try:\n"
            << "            x_train, y_train = _load_mnist_csv(train_source)\n"
            << "            x_test,  y_test  = _load_mnist_csv(test_source)\n"
            << "            return (x_train, y_train), (x_test, y_test)\n"
            << "        except Exception as e:\n"
            << "            _log(\n"
            << "                f\"Failed to load real datasets from CSV; "
               "falling back to random: {e}\"\n"
            << "            )\n"
            << "\n"
            << "    _log(\"Creating random MNIST-like data (JAX stub)\")\n"
            << "    return create_random_mnist_like_data()\n"
            << "\n";
    }

    return out.str();
}

// ----------------------------------------------------------------------------
// Model section (CNN + MLP, arch-aware factory, optax optimizer)
// ----------------------------------------------------------------------------

std::string JaxBackend::emitModelSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Model definitions (CNN + MLP, JAX + optax)\n"
        << "# ============================================================\n";

    if (module.models.empty()) {
        out << "_log(\"No models defined in this module.\")\n";
        return out.str();
    }

    // ---------------------------------------------------------------------
    // 1) CNN utilities using lax.conv_general_dilated + max-pooling
    //     - Input layout: NHWC (N, H, W, C) = (N, 28, 28, in_channels)
    //     - We mirror the structure of SimpleCNN in the PyTorch backend:
    //         Conv2d -> ReLU -> MaxPool2d
    //         Conv2d -> ReLU -> MaxPool2d
    //         Flatten -> Linear -> ReLU -> Linear
    // ---------------------------------------------------------------------
    out << "from jax import lax\n"
        << "\n"
        << "def _max_pool_2x2_nhwc(x: jax.Array) -> jax.Array:\n"
        << "    return lax.reduce_window(\n"
        << "        x,\n"
        << "        -jnp.inf,\n"
        << "        lax.max,\n"
        << "        window_dimensions=(1, 2, 2, 1),\n"
        << "        window_strides=(1, 2, 2, 1),\n"
        << "        padding=\"SAME\",\n"
        << "    )\n"
        << "\n"
        << "def init_cnn_params(key: KeyArray,\n"
        << "                    in_channels: int = 1,\n"
        << "                    num_classes: int = 10) -> Dict[str, Any]:\n"
        << "    \"\"\"Initialize a small CNN's parameters.\n"
        << "\n"
        << "    Layout: NHWC, filters in HWIO.\n"
        << "    - conv1: in_channels -> 32\n"
        << "    - conv2: 32 -> 64\n"
        << "    After two 2x2 pools on 28x28, we get approx 7x7 spatial.\n"
        << "    \"\"\"\n"
        << "    k1, k2, k3, k4 = jax.random.split(key, 4)\n"
        << "\n"
        << "    conv1_w = jax.random.normal(k1, (3, 3, in_channels, 32)) * "
           "0.1\n"
        << "    conv1_b = jnp.zeros((32,))\n"
        << "\n"
        << "    conv2_w = jax.random.normal(k2, (3, 3, 32, 64)) * 0.1\n"
        << "    conv2_b = jnp.zeros((64,))\n"
        << "\n"
        << "    # Flatten 7x7x64 = 3136 for MNIST-like\n"
        << "    fc1_w = jax.random.normal(k3, (7 * 7 * 64, 128)) * 0.1\n"
        << "    fc1_b = jnp.zeros((128,))\n"
        << "\n"
        << "    fc2_w = jax.random.normal(k4, (128, num_classes)) * 0.1\n"
        << "    fc2_b = jnp.zeros((num_classes,))\n"
        << "\n"
        << "    return {\n"
        << "        \"conv1_w\": conv1_w,\n"
        << "        \"conv1_b\": conv1_b,\n"
        << "        \"conv2_w\": conv2_w,\n"
        << "        \"conv2_b\": conv2_b,\n"
        << "        \"fc1_w\": fc1_w,\n"
        << "        \"fc1_b\": fc1_b,\n"
        << "        \"fc2_w\": fc2_w,\n"
        << "        \"fc2_b\": fc2_b,\n"
        << "    }\n"
        << "\n"
        << "def cnn_apply(params: Dict[str, Any], x: jax.Array, *, train: "
           "bool) -> jax.Array:\n"
        << "    # x: (N, H, W, C)\n"
        << "    w1 = params[\"conv1_w\"]\n"
        << "    b1 = params[\"conv1_b\"]\n"
        << "    w2 = params[\"conv2_w\"]\n"
        << "    b2 = params[\"conv2_b\"]\n"
        << "    w3 = params[\"fc1_w\"]\n"
        << "    b3 = params[\"fc1_b\"]\n"
        << "    w4 = params[\"fc2_w\"]\n"
        << "    b4 = params[\"fc2_b\"]\n"
        << "\n"
        << "    # Conv1\n"
        << "    x = lax.conv_general_dilated(\n"
        << "        x,\n"
        << "        w1,\n"
        << "        window_strides=(1, 1),\n"
        << "        padding=\"SAME\",\n"
        << "        dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\"),\n"
        << "    )\n"
        << "    x = x + b1\n"
        << "    x = jax.nn.relu(x)\n"
        << "    x = _max_pool_2x2_nhwc(x)\n"
        << "\n"
        << "    # Conv2\n"
        << "    x = lax.conv_general_dilated(\n"
        << "        x,\n"
        << "        w2,\n"
        << "        window_strides=(1, 1),\n"
        << "        padding=\"SAME\",\n"
        << "        dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\"),\n"
        << "    )\n"
        << "    x = x + b2\n"
        << "    x = jax.nn.relu(x)\n"
        << "    x = _max_pool_2x2_nhwc(x)\n"
        << "\n"
        << "    # Flatten\n"
        << "    x = x.reshape((x.shape[0], -1))\n"
        << "\n"
        << "    # FC layers\n"
        << "    x = jax.nn.relu(x @ w3 + b3)\n"
        << "    logits = x @ w4 + b4\n"
        << "    return logits\n"
        << "\n";

    // ---------------------------------------------------------------------
    // 2) MLP utilities
    // ---------------------------------------------------------------------
    out << "def init_mlp_params(key: KeyArray,\n"
        << "                    input_dim: int = 784,\n"
        << "                    hidden_dim: int = 256,\n"
        << "                    num_layers: int = 2,\n"
        << "                    num_classes: int = 10,\n"
        << "                    dropout: float = 0.0) -> Dict[str, Any]:\n"
        << "    \"\"\"Initialize a simple fully-connected MLP.\n"
        << "\n"
        << "    We build (num_layers - 1) hidden layers + final classifier.\n"
        << "    Dropout is included as a hyperparameter, but for simplicity\n"
        << "    we only use it as a no-op here (place to extend later).\n"
        << "    \"\"\"\n"
        << "    keys = jax.random.split(key, max(2, num_layers + 1))\n"
        << "    params: Dict[str, Any] = {}\n"
        << "    dim_in = input_dim\n"
        << "    # Hidden layers\n"
        << "    for i in range(max(1, num_layers - 1)):\n"
        << "        k_w, k_b = jax.random.split(keys[i])\n"
        << "        w = jax.random.normal(k_w, (dim_in, hidden_dim)) * 0.01\n"
        << "        b = jnp.zeros((hidden_dim,))\n"
        << "        params[f\"layer{i}_w\"] = w\n"
        << "        params[f\"layer{i}_b\"] = b\n"
        << "        dim_in = hidden_dim\n"
        << "\n"
        << "    # Final classification layer\n"
        << "    k_w, k_b = jax.random.split(keys[-1])\n"
        << "    w = jax.random.normal(k_w, (dim_in, num_classes)) * 0.01\n"
        << "    b = jnp.zeros((num_classes,))\n"
        << "    params[\"out_w\"] = w\n"
        << "    params[\"out_b\"] = b\n"
        << "    # NOTE: we deliberately do NOT store num_layers/dropout "
           "inside\n"
        << "    # the params pytree. They are hyperparameters, not trainable\n"
        << "    # weights, and keeping params purely as weights/biases avoids\n"
        << "    # JAX grad / tracing issues.\n"
        << "    return params\n"
        << "\n"
        << "def mlp_apply(params: Dict[str, Any], x: jax.Array, *, train: "
           "bool) -> jax.Array:\n"
        << "    # Flatten if image-like (N, H, W, C)\n"
        << "    if x.ndim > 2:\n"
        << "        x = x.reshape((x.shape[0], -1))\n"
        << "    # Infer how many hidden layers we have by inspecting the "
           "keys.\n"
        << "    # We created them as 'layer0_w', 'layer0_b', ..., so we just\n"
        << "    # count how many consecutive layer<i>_w entries exist.\n"
        << "    num_hidden = 0\n"
        << "    while f\"layer{num_hidden}_w\" in params:\n"
        << "        num_hidden += 1\n"
        << "    for i in range(num_hidden):\n"
        << "        w = params[f\"layer{i}_w\"]\n"
        << "        b = params[f\"layer{i}_b\"]\n"
        << "        x = jax.nn.relu(x @ w + b)\n"
        << "        # Dropout could be applied here if desired (using train "
           "flag)\n"
        << "    w_out = params[\"out_w\"]\n"
        << "    b_out = params[\"out_b\"]\n"
        << "    logits = x @ w_out + b_out\n"
        << "    return logits\n"
        << "\n";

    // ---------------------------------------------------------------------
    // 3) Optimizer + train/eval helpers
    // ---------------------------------------------------------------------
    out << "def make_optimizer(optimizer_name: str, lr: float):\n"
        << "    name = optimizer_name.lower()\n"
        << "    if name == \"sgd\":\n"
        << "        return optax.sgd(lr, momentum=0.9)\n"
        << "    elif name == \"adamw\":\n"
        << "        return optax.adamw(lr)\n"
        << "    else:\n"
        << "        return optax.adam(lr)\n"
        << "\n"
        << "def make_train_step(optimizer, apply_fn, num_classes: int):\n"
        << "    \"\"\"Create a JITed train_step for a given model + "
           "optimizer.\"\"\"\n"
        << "    @jax.jit\n"
        << "    def train_step(params, opt_state, x, y):\n"
        << "        def loss_fn(p):\n"
        << "            logits = apply_fn(p, x, train=True)\n"
        << "            one_hot = jax.nn.one_hot(y, num_classes)\n"
        << "            loss = -jnp.mean(\n"
        << "                jnp.sum(one_hot * jax.nn.log_softmax(logits), "
           "axis=-1)\n"
        << "            )\n"
        << "            return loss\n"
        << "\n"
        << "        loss, grads = jax.value_and_grad(loss_fn)(params)\n"
        << "        updates, opt_state = optimizer.update(grads, opt_state, "
           "params)\n"
        << "        params = optax.apply_updates(params, updates)\n"
        << "        return params, opt_state, loss\n"
        << "\n"
        << "    return train_step\n"
        << "\n"
        << "def make_eval_fn(apply_fn, num_classes: int):\n"
        << "    def eval_fn(params, x, y) -> Tuple[float, float]:\n"
        << "        logits = apply_fn(params, x, train=False)\n"
        << "        preds = jnp.argmax(logits, axis=-1)\n"
        << "        acc = jnp.mean((preds == y).astype(jnp.float32))\n"
        << "        one_hot = jax.nn.one_hot(y, num_classes)\n"
        << "        loss = -jnp.mean(\n"
        << "            jnp.sum(one_hot * jax.nn.log_softmax(logits), "
           "axis=-1)\n"
        << "        )\n"
        << "        return float(acc), float(loss)\n"
        << "    return eval_fn\n"
        << "\n";

    // ---------------------------------------------------------------------
    // 4) Emit a factory for each model in the IR
    // ---------------------------------------------------------------------
    for (const auto &m : module.models) {
        std::string lr = getNumericField(m.fields, "lr", "1e-3");
        std::string epochs = getNumericField(m.fields, "epochs", "10");
        std::string optName = getStringField(m.fields, "optimizer", "adam");

        std::string arch = getStringField(m.fields, "arch", "cnn");

        // --- C++-side normalization: collapse all cnn* -> "cnn", mlp* -> "mlp"
        // ---
        if (!arch.empty()) {
            std::string lowerArch = arch;
            std::transform(lowerArch.begin(), lowerArch.end(),
                           lowerArch.begin(),
                           [](unsigned char c) { return std::tolower(c); });

            // cnn_small, cnn_deep, cnn_grid, cnn_eval, cnn_v1, cnn_v2, ...
            if (lowerArch.rfind("cnn", 0) == 0) {
                arch = "cnn";
            } else if (lowerArch.rfind("mlp", 0) == 0) {
                arch = "mlp";
            }
        }

        std::string inChannels = getNumericField(m.fields, "in_channels", "1");
        std::string numClasses = getNumericField(m.fields, "num_classes", "10");

        std::string inputDim = getNumericField(m.fields, "input_dim", "784");
        std::string hiddenDim = getNumericField(m.fields, "hidden_dim", "256");
        std::string mlpLayers = getNumericField(m.fields, "mlp_layers", "2");
        std::string dropout = getNumericField(m.fields, "dropout", "0.0");

        std::string funcName = "create_" + m.name;

        out << "def " << funcName << "():\n"
            << "    \"\"\"Create model '" << m.name
            << "' (CNN or MLP) and its training utilities.\n"
            << "\n"
            << "    Returns:\n"
            << "        params, opt_state, num_epochs, train_step, eval_fn\n"
            << "    where train_step and eval_fn are callables.\n"
            << "    \"\"\"\n"
            << "    arch = \"" << arch << "\".lower()\n"
            << "    # Normalize CNN family arches: cnn, cnn_small, cnn_deep,\n"
            << "    # cnn_grid, cnn_eval, cnn_v1, cnn_v2, cnn_distributed, "
               "...\n"
            << "    if arch.startswith(\"cnn\"):\n"
            << "        arch = \"cnn\"\n"
            << "    in_channels = " << inChannels << "\n"
            << "    num_classes = " << numClasses << "\n"
            << "    input_dim = " << inputDim << "\n"
            << "    hidden_dim = " << hiddenDim << "\n"
            << "    mlp_layers = " << mlpLayers << "\n"
            << "    dropout = " << dropout << "\n"
            << "    lr = " << lr << "\n"
            << "    optimizer_name = \"" << optName << "\"\n"
            << "    num_epochs = " << epochs << "\n"
            << "\n"
            << "    key = jax.random.PRNGKey(0)\n"
            << "\n"
            << "    if arch == \"cnn\":\n"
            << "        params = init_cnn_params(key, "
               "in_channels=in_channels,\n"
            << "                                  num_classes=num_classes)\n"
            << "        apply_fn = lambda p, x, train: cnn_apply(p, x, "
               "train=train)\n"
            << "    elif arch == \"mlp\":\n"
            << "        params = init_mlp_params(\n"
            << "            key,\n"
            << "            input_dim=input_dim,\n"
            << "            hidden_dim=hidden_dim,\n"
            << "            num_layers=mlp_layers,\n"
            << "            num_classes=num_classes,\n"
            << "            dropout=dropout,\n"
            << "        )\n"
            << "        apply_fn = lambda p, x, train: mlp_apply(p, x, "
               "train=train)\n"
            << "    else:\n"
            << "        _log(\n"
            << "            f\"Unknown arch '{arch}', falling back to CNN with "
               "in_channels={in_channels}, num_classes={num_classes}\"\n"
            << "        )\n"
            << "        params = init_cnn_params(key, "
               "in_channels=in_channels,\n"
            << "                                  num_classes=num_classes)\n"
            << "        apply_fn = lambda p, x, train: cnn_apply(p, x, "
               "train=train)\n"
            << "\n"
            << "    optimizer = make_optimizer(optimizer_name, lr)\n"
            << "    opt_state = optimizer.init(params)\n"
            << "    train_step = make_train_step(optimizer, apply_fn, "
               "num_classes)\n"
            << "    eval_fn = make_eval_fn(apply_fn, num_classes)\n"
            << "\n"
            << "    return params, opt_state, num_epochs, train_step, eval_fn\n"
            << "\n";
    }

    return out.str();
}

// ----------------------------------------------------------------------------
// Experiment section â€“ smart resolution of dataset + model (like PyTorch)
// ----------------------------------------------------------------------------

std::string
JaxBackend::emitExperimentSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Experiment pipelines (JAX training loop)\n"
        << "# ============================================================\n";

    if (module.experiments.empty()) {
        out << "_log(\"No experiments defined in this module.\")\n";
        return out.str();
    }

    // For each experiment in the IR, create a run_experiment_<name>().
    for (const auto &e : module.experiments) {
        const std::string &expName = e.name;

        // ==============================================================
        // 1) Infer dataset + model from the IR ops
        // ==============================================================

        // Start with experiment parameter as a default dataset
        std::string chosenDatasetName = e.datasetParam; // e.g. "mnist"
        std::string chosenModelName; // "simple_cnn" / "mnist_mlp"

        // Prefer the first Train op
        for (const auto &op : e.ops) {
            if (op.kind == PipelineOpKind::Train) {
                chosenModelName = op.train.modelName;
                chosenDatasetName = op.train.datasetName;
                break;
            }
        }

        // If no Train op, fall back to first Evaluate op
        if (chosenModelName.empty()) {
            for (const auto &op : e.ops) {
                if (op.kind == PipelineOpKind::Evaluate) {
                    chosenModelName = op.eval.modelName;
                    chosenDatasetName = op.eval.datasetName;
                    break;
                }
            }
        }

        // --------------------------------------------------------------
        // Resolve DATASET factory: create_<dataset>_data()
        // --------------------------------------------------------------
        std::string dsFactoryName;
        std::string dsComment;
        std::string batchSizeStr = "64";

        if (module.datasets.empty()) {
            dsComment =
                "ERROR: no datasets defined in this module; experiment '" +
                expName + "' cannot run.";
        } else {
            std::string fallbackDatasetName = module.datasets.front().name;

            // Prefer experiment parameter as fallback if present
            for (const auto &d : module.datasets) {
                if (d.name == e.datasetParam) {
                    fallbackDatasetName = d.name;
                    break;
                }
            }

            std::string candidateDatasetName = chosenDatasetName.empty()
                                                   ? fallbackDatasetName
                                                   : chosenDatasetName;

            bool foundDataset = false;
            const IRDataset *chosenDatasetDecl = nullptr;
            for (const auto &d : module.datasets) {
                if (d.name == candidateDatasetName) {
                    foundDataset = true;
                    chosenDatasetDecl = &d;
                    break;
                }
            }

            if (foundDataset && chosenDatasetDecl) {
                dsFactoryName = "create_" + candidateDatasetName + "_data";
                dsComment = "Dataset chosen from train()/evaluate() or "
                            "experiment parameter: '" +
                            candidateDatasetName + "'";

                // Try to read batch size from the dataset fields
                batchSizeStr =
                    getNumericField(chosenDatasetDecl->fields, "batch", "64");
            } else {
                dsFactoryName = "create_" + fallbackDatasetName + "_data";
                dsComment = "WARNING: dataset '" + candidateDatasetName +
                            "' not found in module; falling back to dataset '" +
                            fallbackDatasetName + "'";

                const IRDataset &fallback = module.datasets.front();
                batchSizeStr = getNumericField(fallback.fields, "batch", "64");
            }
        }

        // --------------------------------------------------------------
        // Resolve MODEL factory: create_<model>()
        // --------------------------------------------------------------
        std::string modelFactory;
        std::string modelComment;

        if (module.models.empty()) {
            modelComment =
                "ERROR: no models defined in this module; experiment '" +
                expName + "' cannot run.";
        } else {
            std::string fallbackModelName = module.models.front().name;
            modelFactory = "create_" + fallbackModelName;

            if (!chosenModelName.empty()) {
                bool foundModel = false;
                for (const auto &m : module.models) {
                    if (m.name == chosenModelName) {
                        foundModel = true;
                        break;
                    }
                }
                if (foundModel) {
                    modelFactory = "create_" + chosenModelName;
                    modelComment =
                        "Model chosen from train()/evaluate() op: '" +
                        chosenModelName + "'";
                } else {
                    modelComment =
                        "WARNING: model '" + chosenModelName +
                        "' not found in module; falling back to first model '" +
                        fallbackModelName + "'";
                }
            } else {
                modelComment = "WARNING: no explicit model in "
                               "train()/evaluate(); using first model '" +
                               fallbackModelName + "'";
            }
        }

        // ==============================================================
        // 2) Emit the run_experiment_<name>() Python function
        // ==============================================================

        out << "def run_experiment_" << expName << "():\n"
            << "    _log(\"Running experiment '" << expName
            << "' (JAX backend)\")\n";

        // Dataset side
        if (module.datasets.empty()) {
            out << "    # " << dsComment << "\n"
                << "    raise RuntimeError(\"[Syntra-JAX] " << dsComment
                << "\")\n\n";
            continue;
        } else {
            out << "    # " << dsComment << "\n"
                << "    (x_train, y_train), (x_test, y_test) = "
                << dsFactoryName << "()\n";
        }

        // Model side
        if (module.models.empty()) {
            out << "    # " << modelComment << "\n"
                << "    raise RuntimeError(\"[Syntra-JAX] " << modelComment
                << "\")\n\n";
            continue;
        } else {
            out << "    # " << modelComment << "\n"
                << "    params, opt_state, num_epochs, train_step, eval_fn = "
                << modelFactory << "()\n"
                << "\n"
                << "    metrics: Dict[str, Any] = {}\n"
                << "\n";
        }

        // Training hyperparams
        out << "    batch_size = " << batchSizeStr << "\n"
            << "    num_train = x_train.shape[0]\n"
            << "    steps_per_epoch = max(1, num_train // batch_size)\n"
            << "\n"
            << "    for epoch in range(num_epochs):\n"
            << "        epoch_loss = 0.0\n"
            << "        for step in range(steps_per_epoch):\n"
            << "            start = step * batch_size\n"
            << "            end = start + batch_size\n"
            << "            xb = x_train[start:end]\n"
            << "            yb = y_train[start:end]\n"
            << "            params, opt_state, loss = train_step(\n"
            << "                params, opt_state, xb, yb\n"
            << "            )\n"
            << "            epoch_loss += float(loss)\n"
            << "        epoch_loss /= steps_per_epoch\n"
            << "        _log(\n"
            << "            f\"[epoch {epoch+1}/{num_epochs}] train loss = "
               "{epoch_loss:.4f}\"\n"
            << "        )\n"
            << "\n"
            << "    acc, loss = eval_fn(params, x_test, y_test)\n"
            << "    _log(f\"eval accuracy={acc:.4f}, loss={loss:.4f}\")\n"
            << "\n"
            << "    result: Dict[str, Any] = {}\n";

        // Wire experiment result mapping
        if (!e.results.empty()) {
            for (const auto &rf : e.results) {
                if (rf.metricName == "accuracy") {
                    out << "    result[\"" << rf.fieldName << "\"] = acc\n";
                } else if (rf.metricName == "loss") {
                    out << "    result[\"" << rf.fieldName << "\"] = loss\n";
                } else {
                    out << "    # Unknown metric '" << rf.metricName
                        << "'; using 0.0 placeholder.\n"
                        << "    result[\"" << rf.fieldName << "\"] = 0.0\n";
                }
            }
        } else {
            out << "    result[\"accuracy\"] = acc\n"
                << "    result[\"loss\"] = loss\n";
        }

        out << "    return result\n\n";
    }

    return out.str();
}

// ----------------------------------------------------------------------------
// Main section
// ----------------------------------------------------------------------------

std::string
JaxBackend::emitMainSection(const PipelineModule &module,
                            const std::string &selectedExperimentName) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Entry point\n"
        << "# ============================================================\n"
        << "if __name__ == \"__main__\":\n";

    if (module.experiments.empty()) {
        out << "    _log(\"No experiments to run.\")\n";
        return out.str();
    }

    const IRExperimentPipeline *chosen = nullptr;

    if (!selectedExperimentName.empty()) {
        for (const auto &e : module.experiments) {
            if (e.name == selectedExperimentName) {
                chosen = &e;
                break;
            }
        }
        if (!chosen) {
            out << "    _log(\"[warning] requested experiment '"
                << selectedExperimentName
                << "' not found; falling back to first experiment.\")\n";
            chosen = &module.experiments.front();
        }
    } else {
        chosen = &module.experiments.front();
    }

    out << "    result = run_experiment_" << chosen->name << "()\n"
        << "    _log(\"Final experiment result (JAX):\")\n"
        << "    print(result)\n";

    return out.str();
}

} // namespace syntra
