#include "syntra/backend_pytorch.hpp"

#include <algorithm>
#include <cctype>
#include <sstream>

namespace syntra {

// Small helpers to read AST literal fields
static const Expr *unwrapLiteral(const Expr &e) { return &e; }

std::string
PytorchBackend::getStringField(const std::vector<const NamedField *> &fields,
                               const std::string &name,
                               const std::string &defaultValue) const {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (f->value->kind == ExprKind::StringLiteral) {
            const auto &s = static_cast<const StringLiteralExpr &>(*f->value);
            return s.value;
        }
    }
    return defaultValue;
}

std::string
PytorchBackend::getNumericField(const std::vector<const NamedField *> &fields,
                                const std::string &name,
                                const std::string &defaultValue) const {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (f->value->kind == ExprKind::IntegerLiteral) {
            const auto &i = static_cast<const IntegerLiteralExpr &>(*f->value);
            return i.value;
        }
        if (f->value->kind == ExprKind::FloatLiteral) {
            const auto &fl = static_cast<const FloatLiteralExpr &>(*f->value);
            return fl.value;
        }
    }
    return defaultValue;
}

std::string
PytorchBackend::getBoolField(const std::vector<const NamedField *> &fields,
                             const std::string &name,
                             const std::string &defaultValue) const {
    for (const auto *f : fields) {
        if (!f)
            continue;
        if (f->name != name)
            continue;
        if (f->value->kind == ExprKind::BoolLiteral) {
            const auto &b = static_cast<const BoolLiteralExpr &>(*f->value);
            return b.value ? "True" : "False";
        }
    }
    return defaultValue;
}

// ============================================================================
// Top-level entry
// ============================================================================

std::string
PytorchBackend::emitPythonScript(const PipelineModule &module,
                                 const std::string &selectedExperimentName) {
    std::ostringstream out;

    out << emitHeader();
    out << "\n\n";
    out << emitDatasetSection(module);
    out << "\n\n";
    out << emitModelSection(module);
    out << "\n\n";
    out << emitExperimentSection(module);
    out << "\n\n";
    out << emitMainSection(module, selectedExperimentName);

    return out.str();
}

// ============================================================================
// Sections
// ============================================================================

std::string PytorchBackend::emitHeader() const {
    std::ostringstream out;

    out << "# Auto-generated by SyntraLine++ PyTorch backend\n"
        << "# This script is a starting point and may require\n"
        << "# manual adjustments for real training.\n"
        << "\n"
        << "import os\n"
        << "import csv\n"
        << "from typing import Any, Dict\n"
        << "\n"
        << "import torch\n"
        << "from torch import nn\n"
        << "from torch.utils.data import Dataset, DataLoader\n"
        << "\n"
        << "# Decide whether to *try* loading real datasets.\n"
        << "# This mirrors the C++ config flag SYNTRA_PREFER_REAL_DATASETS.\n"
        << "USE_REAL_DATA = os.environ.get(\"SYNTRA_PREFER_REAL_DATASETS\", "
           "\"0\").lower() in (\n"
           "    \"1\", \"true\", \"yes\", \"on\"\n"
        << ")\n"
        << "\n"
        << "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n";

    return out.str();
}

std::string
PytorchBackend::emitDatasetSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Dataset definitions (real CSV loader + random fallback)\n"
        << "# ============================================================\n";

    if (module.datasets.empty()) {
        out << "# No datasets defined in this module.\n";
        return out.str();
    }

    // ---------------------------------------------------------------------
    // SyntraImageDataset: same as you already have (real CSV + random fallback)
    // ---------------------------------------------------------------------
    out << "class SyntraImageDataset(Dataset):\n"
        << "    \"\"\"MNIST-like dataset.\n"
        << "\n"
        << "    Behavior:\n"
        << "      - If USE_REAL_DATA is True and `source` exists, we *try* to\n"
        << "        interpret it as an MNIST-style CSV:\n"
        << "            label, p0, p1, ..., p783\n"
        << "        where pixels are 0â€“255. On success, we store all samples "
           "in\n"
        << "        memory as 1x28x28 tensors.\n"
        << "      - On any failure (file missing, parse error, weird shape), "
           "we\n"
        << "        fall back to the original random-tensor stub.\n"
        << "    \"\"\"\n"
        << "\n"
        << "    def __init__(self, source: str, split: str = \"train\") -> "
           "None:\n"
        << "        self.source = source\n"
        << "        self.split = split\n"
        << "\n"
        << "        # Default: random data stub (old behavior)\n"
        << "        self._size = 1000 if split == \"train\" else 200\n"
        << "        self._use_random = True\n"
        << "        self._data = None\n"
        << "        self._labels = None\n"
        << "\n"
        << "        if USE_REAL_DATA and os.path.exists(source):\n"
        << "            try:\n"
        << "                xs = []\n"
        << "                ys = []\n"
        << "                with open(source, \"r\") as f:\n"
        << "                    reader = csv.reader(f)\n"
        << "                    for row in reader:\n"
        << "                        if not row:\n"
        << "                            continue\n"
        << "                        # Assume: first column is label, rest "
           "pixels\n"
        << "                        label = int(row[0])\n"
        << "                        pixels = [float(v) / 255.0 for v in "
           "row[1:]]\n"
        << "                        if len(pixels) != 28 * 28:\n"
        << "                            # Shape mismatch -> bail out to "
           "random.\n"
        << "                            raise ValueError(\n"
        << "                                f\"expected 784 pixels per row, "
           "got {len(pixels)}\"\n"
        << "                            )\n"
        << "                        x = torch.tensor(pixels, "
           "dtype=torch.float32).view(1, 28, 28)\n"
        << "                        xs.append(x)\n"
        << "                        ys.append(label)\n"
        << "\n"
        << "                if xs:\n"
        << "                    self._data = torch.stack(xs, dim=0)\n"
        << "                    self._labels = torch.tensor(ys, "
           "dtype=torch.long)\n"
        << "                    self._size = self._data.shape[0]\n"
        << "                    self._use_random = False\n"
        << "                    print(\n"
        << "                        f\"[Syntra-PyTorch] Loaded real data from "
           "{source} \"\n"
        << "                        f\"({self._size} samples, "
           "{self._data.shape[1:]} each)\"\n"
        << "                    )\n"
        << "                else:\n"
        << "                    print(\n"
        << "                        f\"[Syntra-PyTorch] File {source} was "
           "empty; \"\n"
        << "                        \"falling back to random stub.\"\n"
        << "                    )\n"
        << "            except Exception as e:\n"
        << "                print(\n"
        << "                    f\"[Syntra-PyTorch] Failed to load real "
           "dataset '{source}', \"\n"
        << "                    f\"falling back to random: {e}\"\n"
        << "                )\n"
        << "\n"
        << "    def __len__(self) -> int:\n"
        << "        return self._size\n"
        << "\n"
        << "    def __getitem__(self, idx: int):\n"
        << "        if not self._use_random and self._data is not None:\n"
        << "            x = self._data[idx]\n"
        << "            y = self._labels[idx].item()\n"
        << "            return x, y\n"
        << "\n"
        << "        # Fallback: dummy 1x28x28 image and label\n"
        << "        x = torch.randn(1, 28, 28)\n"
        << "        y = torch.randint(0, 10, (1,)).item()\n"
        << "        return x, y\n"
        << "\n";

    // ---------------------------------------------------------------------
    // Emit dataset loader creator functions
    // ---------------------------------------------------------------------
    for (const auto &d : module.datasets) {
        // 1. Base training source from DSL.
        std::string sourceTrain =
            getStringField(d.fields, "source", "data/unknown.csv");

        // 2. Optional explicit test_source field in DSL.
        std::string sourceTest = getStringField(d.fields, "test_source", "");

        // 3. If test_source not provided, try to derive from train.
        if (sourceTest.empty()) {
            sourceTest = sourceTrain;
            // heuristic: foo_train.csv -> foo_test.csv
            const std::string suffix = "_train.csv";
            if (sourceTrain.size() >= suffix.size()) {
                if (sourceTrain.compare(sourceTrain.size() - suffix.size(),
                                        suffix.size(), suffix) == 0) {
                    sourceTest = sourceTrain.substr(0, sourceTrain.size() -
                                                           suffix.size()) +
                                 "_test.csv";
                }
            }
        }

        std::string batch = getNumericField(d.fields, "batch", "64");
        std::string shuffle = getBoolField(d.fields, "shuffle", "True");

        std::string dsFuncName = "create_" + d.name + "_dataloaders";

        out << "def " << dsFuncName << "() -> Dict[str, DataLoader]:\n"
            << "    \"\"\"Create train/test dataloaders for dataset '" << d.name
            << "'.\"\"\"\n"
            << "    source_train = \"" << sourceTrain << "\"\n"
            << "    source_test  = \"" << sourceTest << "\"\n"
            << "    batch_size = " << batch << "\n"
            << "    shuffle_train = " << shuffle << "\n"
            << "\n"
            << "    print(f\"[Syntra-PyTorch] Using train source: "
               "{source_train}\")\n"
            << "    print(f\"[Syntra-PyTorch] Using test  source: "
               "{source_test}\")\n"
            << "\n"
            << "    train_ds = SyntraImageDataset(source_train, "
               "split=\"train\")\n"
            << "    test_ds  = SyntraImageDataset(source_test,  "
               "split=\"test\")\n"
            << "\n"
            << "    return {\n"
            << "        \"train\": DataLoader(train_ds, batch_size=batch_size, "
               "shuffle=shuffle_train),\n"
            << "        \"test\":  DataLoader(test_ds,  batch_size=batch_size, "
               "shuffle=False),\n"
            << "    }\n\n";
    }

    return out.str();
}

std::string
PytorchBackend::emitModelSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Model definitions (SimpleCNN + SimpleMLP + arch-aware factory)\n"
        << "# ============================================================\n";

    if (module.models.empty()) {
        out << "# No models defined in this module.\n";
        return out.str();
    }

    // ---------------------------------------------------------------------
    // 1) Convolutional model: SimpleCNN (UNCHANGED behavior)
    // ---------------------------------------------------------------------
    out << "class SimpleCNN(nn.Module):\n"
        << "    def __init__(self, in_channels: int = 1, num_classes: int = "
           "10) -> None:\n"
        << "        super().__init__()\n"
        << "        self.features = nn.Sequential(\n"
        << "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n"
        << "            nn.ReLU(),\n"
        << "            nn.MaxPool2d(2),\n"
        << "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n"
        << "            nn.ReLU(),\n"
        << "            nn.MaxPool2d(2),\n"
        << "        )\n"
        << "        # NOTE: We assume input is at least 28x28 so after two "
           "MaxPool(2)\n"
        << "        #       we get 64 x 7 x 7. For other sizes, you can switch "
           "this to\n"
        << "        #       an AdaptiveAvgPool2d or change the classifier.\n"
        << "        self.classifier = nn.Sequential(\n"
        << "            nn.Flatten(),\n"
        << "            nn.Linear(64 * 7 * 7, 128),\n"
        << "            nn.ReLU(),\n"
        << "            nn.Linear(128, num_classes),\n"
        << "        )\n"
        << "\n"
        << "    def forward(self, x):\n"
        << "        x = self.features(x)\n"
        << "        x = self.classifier(x)\n"
        << "        return x\n"
        << "\n";

    // ---------------------------------------------------------------------
    // 2) New: SimpleMLP for 'mlp' architectures
    // ---------------------------------------------------------------------
    out << "class SimpleMLP(nn.Module):\n"
        << "    def __init__(\n"
        << "        self,\n"
        << "        input_dim: int = 784,\n"
        << "        num_classes: int = 10,\n"
        << "        hidden_dim: int = 256,\n"
        << "        num_layers: int = 2,\n"
        << "        dropout: float = 0.0,\n"
        << "    ) -> None:\n"
        << "        super().__init__()\n"
        << "        layers = []\n"
        << "        dim_in = input_dim\n"
        << "\n"
        << "        # Build (num_layers - 1) hidden layers + final classifier\n"
        << "        for i in range(max(1, num_layers - 1)):\n"
        << "            layers.append(nn.Linear(dim_in, hidden_dim))\n"
        << "            layers.append(nn.ReLU())\n"
        << "            if dropout > 0.0:\n"
        << "                layers.append(nn.Dropout(dropout))\n"
        << "            dim_in = hidden_dim\n"
        << "\n"
        << "        # Final classification layer\n"
        << "        layers.append(nn.Linear(dim_in, num_classes))\n"
        << "        self.net = nn.Sequential(*layers)\n"
        << "\n"
        << "    def forward(self, x):\n"
        << "        # If input is image-like (N, C, H, W), flatten to (N, D)\n"
        << "        if x.dim() > 2:\n"
        << "            x = x.view(x.size(0), -1)\n"
        << "        return self.net(x)\n"
        << "\n";

    // ---------------------------------------------------------------------
    // 3) Emit a factory for each model in the IR
    // ---------------------------------------------------------------------
    for (const auto &m : module.models) {
        // Shared hyperparameters
        std::string lr = getNumericField(m.fields, "lr", "1e-3");
        std::string epochs = getNumericField(m.fields, "epochs", "10");
        std::string optName = getStringField(m.fields, "optimizer", "adam");

        // Architecture & common classification fields
        std::string arch = getStringField(m.fields, "arch", "cnn");

        // --- C++-side normalization: collapse all cnn* -> "cnn", mlp* -> "mlp"
        // ---
        if (!arch.empty()) {
            std::string lowerArch = arch;
            std::transform(lowerArch.begin(), lowerArch.end(),
                           lowerArch.begin(),
                           [](unsigned char c) { return std::tolower(c); });

            // cnn_small, cnn_deep, cnn_grid, cnn_eval, cnn_v1, cnn_v2, ...
            if (lowerArch.rfind("cnn", 0) == 0) {
                arch = "cnn";
            } else if (lowerArch.rfind("mlp", 0) == 0) {
                arch = "mlp";
            }
        }

        std::string inChannels = getNumericField(m.fields, "in_channels", "1");
        std::string numClasses = getNumericField(m.fields, "num_classes", "10");

        // MLP-specific fields (used when arch == "mlp")
        std::string inputDim = getNumericField(m.fields, "input_dim", "784");
        std::string hiddenDim = getNumericField(m.fields, "hidden_dim", "256");
        std::string mlpLayers = getNumericField(m.fields, "mlp_layers", "2");
        std::string dropout = getNumericField(m.fields, "dropout", "0.0");

        std::string funcName = "create_" + m.name;

        out << "def " << funcName << "():\n"
            << "    \"\"\"Create model '" << m.name
            << "' and its optimizer/schedule.\"\"\"\n"
            << "    arch = \"" << arch << "\".lower()\n"
            << "    # Normalize CNN family arches: cnn, cnn_small, cnn_deep,\n"
            << "    # cnn_grid, cnn_eval, cnn_v1, cnn_v2, cnn_distributed, "
               "...\n"
            << "    if arch.startswith(\"cnn\"):\n"
            << "        arch = \"cnn\"\n"
            << "    in_channels = " << inChannels << "\n"
            << "    num_classes = " << numClasses << "\n"
            << "    input_dim = " << inputDim << "\n"
            << "    hidden_dim = " << hiddenDim << "\n"
            << "    mlp_layers = " << mlpLayers << "\n"
            << "    dropout = " << dropout << "\n"
            << "\n"
            << "    # Select architecture.\n"
            << "    if arch == \"cnn\":\n"
            << "        model = SimpleCNN(\n"
            << "            in_channels=in_channels,\n"
            << "            num_classes=num_classes,\n"
            << "        ).to(DEVICE)\n"
            << "    elif arch == \"mlp\":\n"
            << "        model = SimpleMLP(\n"
            << "            input_dim=input_dim,\n"
            << "            num_classes=num_classes,\n"
            << "            hidden_dim=hidden_dim,\n"
            << "            num_layers=mlp_layers,\n"
            << "            dropout=dropout,\n"
            << "        ).to(DEVICE)\n"
            << "    else:\n"
            << "        print(\n"
            << "            f\"[Syntra-PyTorch] Unknown arch '{arch}', "
               "falling back to SimpleCNN\"\n"
            << "        )\n"
            << "        model = SimpleCNN(\n"
            << "            in_channels=in_channels,\n"
            << "            num_classes=num_classes,\n"
            << "        ).to(DEVICE)\n"
            << "\n"
            << "    lr = " << lr << "\n"
            << "    optimizer_name = \"" << optName << "\".lower()\n"
            << "    if optimizer_name == \"sgd\":\n"
            << "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, "
               "momentum=0.9)\n"
            << "    elif optimizer_name == \"adamw\":\n"
            << "        optimizer = torch.optim.AdamW(model.parameters(), "
               "lr=lr)\n"
            << "    else:\n"
            << "        optimizer = torch.optim.Adam(model.parameters(), "
               "lr=lr)\n"
            << "\n"
            << "    num_epochs = " << epochs << "\n"
            << "    return model, optimizer, num_epochs\n\n";
    }

    return out.str();
}

// Helper to emit a specialized experiment function for `compare_versions`
// that trains/evaluates two (model, dataset) pairs separately.
static void
emitCompareVersionsExperimentPyTorch(std::ostringstream &out,
                                     const PipelineModule &module,
                                     const IRExperimentPipeline &e) {

    // We expect something like:
    //   train(simple_cnn_v1, mnist_v1.train)
    //   evaluate(simple_cnn_v1, mnist_v1.test)
    //   train(simple_cnn_v2, mnist_v2.train)
    //   evaluate(simple_cnn_v2, mnist_v2.test)

    struct EvalPair {
        std::string modelName;
        std::string datasetName;
    };
    std::vector<EvalPair> evalPairs;

    // Collect (model, dataset) pairs from Evaluate ops in order.
    for (const auto &op : e.ops) {
        if (op.kind == PipelineOpKind::Evaluate) {
            EvalPair p;
            p.modelName = op.eval.modelName;
            p.datasetName = op.eval.datasetName;
            evalPairs.push_back(p);
        }
    }

    if (evalPairs.size() < 2) {
        // Fallback: emit nothing special; caller should not use this in that
        // case.
        out << "def run_experiment_" << e.name << "():\n"
            << "    raise RuntimeError(\"[Syntra-PyTorch] compare_versions "
               "requires at least two evaluate ops.\")\n\n";
        return;
    }

    // Helper lambda to check if dataset/model exists in module
    auto hasDataset = [&](const std::string &name) -> bool {
        for (const auto &d : module.datasets) {
            if (d.name == name)
                return true;
        }
        return false;
    };
    auto hasModel = [&](const std::string &name) -> bool {
        for (const auto &m : module.models) {
            if (m.name == name)
                return true;
        }
        return false;
    };

    // Emit Python function body
    const std::string &expName = e.name;
    out << "def run_experiment_" << expName << "():\n"
        << "    # Specialized versioning experiment: train v1/v2 separately\n"
        << "    # and compare metrics.\n"
        << "    # This is auto-generated to respect the DSL semantics of\n"
        << "    # `compare_versions` in versioning.syntra.\n";

    // For each eval pair (we assume here: index 0 = v1, index 1 = v2)
    for (size_t i = 0; i < evalPairs.size(); ++i) {
        const auto &p = evalPairs[i];
        std::string dsName = p.datasetName;
        std::string modelName = p.modelName;

        // Fallback if names are missing
        if (dsName.empty() && !module.datasets.empty()) {
            dsName = module.datasets.front().name;
        }
        if (modelName.empty() && !module.models.empty()) {
            modelName = module.models.front().name;
        }

        // Sanity check presence; if missing, we fall back but emit a comment
        if (!hasDataset(dsName) && !module.datasets.empty()) {
            out << "    # WARNING: dataset '" << dsName
                << "' not found; falling back to '"
                << module.datasets.front().name << "'\n";
            dsName = module.datasets.front().name;
        }
        if (!hasModel(modelName) && !module.models.empty()) {
            out << "    # WARNING: model '" << modelName
                << "' not found; falling back to '"
                << module.models.front().name << "'\n";
            modelName = module.models.front().name;
        }

        std::string dsFactory = "create_" + dsName + "_dataloaders";
        std::string modelFactory = "create_" + modelName;

        // Python variable suffix: v1 / v2 if we can guess from name
        std::string suffix;
        if (modelName.find("v1") != std::string::npos ||
            dsName.find("v1") != std::string::npos) {
            suffix = "v1";
        } else if (modelName.find("v2") != std::string::npos ||
                   dsName.find("v2") != std::string::npos) {
            suffix = "v2";
        } else {
            // generic index-based suffix (_0, _1, ...)
            suffix = "m" + std::to_string(i);
        }

        // Emit creation + training loop
        out << "    # --- Version " << suffix << " : model '" << modelName
            << "', dataset '" << dsName << "'\n"
            << "    loaders_" << suffix << " = " << dsFactory << "()\n"
            << "    model_" << suffix << ", opt_" << suffix << ", num_epochs_"
            << suffix << " = " << modelFactory << "()\n"
            << "    for epoch in range(num_epochs_" << suffix << "):\n"
            << "        stats_" << suffix << " = train_one_epoch(\n"
            << "            model_" << suffix << ",\n"
            << "            opt_" << suffix << ",\n"
            << "            loaders_" << suffix << "[\"train\"],\n"
            << "        )\n"
            << "        print(\n"
            << "            f\"[version " << suffix
            << "][epoch {epoch+1}/{num_epochs_" << suffix
            << "}] train loss = {stats_" << suffix << "['loss']:.4f}\"\n"
            << "        )\n"
            << "    eval_stats_" << suffix << " = evaluate_model(model_"
            << suffix << ", loaders_" << suffix << "[\"test\"])\n"
            << "    print(\"[version " << suffix
            << "] eval stats:\", eval_stats_" << suffix << ")\n\n";
    }

    // Now wire result dict: acc_v1/loss_v1 from v1 eval, acc_v2/loss_v2 from v2
    // eval
    out << "    result = {}\n";

    // We look at result field names to decide which version they belong to.
    for (const auto &rf : e.results) {
        const std::string &field = rf.fieldName;
        const std::string &metric =
            rf.metricName; // typically "accuracy" or "loss"

        std::string suffixExpr = "v1"; // default
        if (field.find("v2") != std::string::npos) {
            suffixExpr = "v2";
        } else if (field.find("v1") != std::string::npos) {
            suffixExpr = "v1";
        }
        // metric key inside eval_stats dict
        out << "    result[\"" << field << "\"] = eval_stats_" << suffixExpr
            << ".get(\"" << metric << "\", None)\n";
    }

    out << "    return result\n\n";
}

std::string
PytorchBackend::emitExperimentSection(const PipelineModule &module) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Experiment pipelines\n"
        << "# ============================================================\n";

    if (module.experiments.empty()) {
        out << "# No experiments defined in this module.\n";
        return out.str();
    }

    // ---------------------------------------------------------------------
    // Generic training & evaluation helpers (shared by all experiments)
    // ---------------------------------------------------------------------
    out << "def train_one_epoch(model, optimizer, loader):\n"
        << "    model.train()\n"
        << "    total_loss = 0.0\n"
        << "    criterion = nn.CrossEntropyLoss()\n"
        << "    for x, y in loader:\n"
        << "        x = x.to(DEVICE)\n"
        << "        y = y.to(DEVICE)\n"
        << "        optimizer.zero_grad()\n"
        << "        logits = model(x)\n"
        << "        loss = criterion(logits, y)\n"
        << "        loss.backward()\n"
        << "        optimizer.step()\n"
        << "        total_loss += loss.item() * x.size(0)\n"
        << "    return {\"loss\": total_loss / len(loader.dataset)}\n"
        << "\n"
        << "def evaluate_model(model, loader):\n"
        << "    model.eval()\n"
        << "    total_loss = 0.0\n"
        << "    correct = 0\n"
        << "    criterion = nn.CrossEntropyLoss()\n"
        << "    with torch.no_grad():\n"
        << "        for x, y in loader:\n"
        << "            x = x.to(DEVICE)\n"
        << "            y = y.to(DEVICE)\n"
        << "            logits = model(x)\n"
        << "            loss = criterion(logits, y)\n"
        << "            total_loss += loss.item() * x.size(0)\n"
        << "            preds = logits.argmax(dim=1)\n"
        << "            correct += (preds == y).sum().item()\n"
        << "    avg_loss = total_loss / len(loader.dataset)\n"
        << "    accuracy = correct / len(loader.dataset)\n"
        << "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n"
        << "\n";

    // ---------------------------------------------------------------------
    // One run_experiment_* function per IR experiment
    // ---------------------------------------------------------------------
    for (const auto &e : module.experiments) {
        const std::string &expName = e.name;

        // SPECIAL CASE: versioning experiment compare_versions
        if (expName == "compare_versions") {
            emitCompareVersionsExperimentPyTorch(out, module, e);
            continue; // skip generic path for this one
        }

        // -------------------------------------------------------------
        // 1) Dataset factory from the experiment's dataset parameter
        //    experiment cnn_mnist(mnist)  -> create_mnist_dataloaders()
        //    experiment mlp_mnist(mnist_flat) ->
        //    create_mnist_flat_dataloaders()
        // -------------------------------------------------------------
        std::string dsFuncName = "create_" + e.datasetParam + "_dataloaders";

        // -------------------------------------------------------------
        // 2) Determine which *model* this experiment trains:
        //    look at the first Train op in e.ops.
        // -------------------------------------------------------------
        std::string modelNameFromTrain; // e.g. "simple_cnn", "mnist_mlp"
        for (const auto &op : e.ops) {
            if (op.kind == PipelineOpKind::Train) {
                modelNameFromTrain = op.train.modelName;
                break;
            }
        }

        // Compute the factory name and an explanatory Python comment
        std::string modelFactory; // e.g. "create_simple_cnn"
        std::string modelComment; // Python comment text

        if (module.models.empty()) {
            modelFactory.clear();
            modelComment =
                "ERROR: no models defined in this module; experiment '" +
                expName + "' cannot run.";
        } else {
            // There is at least one model in the module.
            // Default fallback is the first one.
            std::string fallbackModelName = module.models.front().name;
            modelFactory = "create_" + fallbackModelName;

            if (!modelNameFromTrain.empty()) {
                bool found = false;
                for (const auto &m : module.models) {
                    if (m.name == modelNameFromTrain) {
                        found = true;
                        break;
                    }
                }

                if (found) {
                    modelFactory = "create_" + modelNameFromTrain;
                    modelComment = "Model chosen from train(...) op: '" +
                                   modelNameFromTrain + "'";
                } else {
                    modelComment =
                        "WARNING: model '" + modelNameFromTrain +
                        "' not found in module; falling back to first model '" +
                        fallbackModelName + "'";
                }
            } else {
                modelComment =
                    "WARNING: no train(...) op found; using first model '" +
                    fallbackModelName + "'";
            }
        }

        // -------------------------------------------------------------
        // 3) Emit the run_experiment_<name>() Python function
        // -------------------------------------------------------------
        out << "def run_experiment_" << expName << "():\n";

        // Dataset side
        out << "    # Dataset factory from experiment parameter '"
            << e.datasetParam << "'\n"
            << "    loaders = " << dsFuncName << "()\n";

        // Model side
        if (module.models.empty()) {
            out << "    # " << modelComment << "\n"
                << "    raise RuntimeError(\"[Syntra-PyTorch] " << modelComment
                << "\")\n\n";
            continue;
        } else {
            out << "    # " << modelComment << "\n"
                << "    model, optimizer, num_epochs = " << modelFactory
                << "()\n"
                << "\n"
                << "    metrics: Dict[str, Any] = {}\n"
                << "\n";
        }

        // -------------------------------------------------------------
        // 4) Replay pipeline ops (train / evaluate)
        // -------------------------------------------------------------
        for (const auto &op : e.ops) {
            if (op.kind == PipelineOpKind::Train) {
                out << "    # Train op: " << op.train.varName << "\n"
                    << "    for epoch in range(num_epochs):\n"
                    << "        train_stats = train_one_epoch(\n"
                    << "            model,\n"
                    << "            optimizer,\n"
                    << "            loaders[\"train\"],\n"
                    << "        )\n"
                    << "        print(\n"
                    << "            f\"[epoch {epoch+1}/{num_epochs}] train "
                       "loss = "
                       "{train_stats['loss']:.4f}\"\n"
                    << "        )\n"
                    << "\n";
            } else if (op.kind == PipelineOpKind::Evaluate) {
                out << "    # Evaluate op: " << op.eval.varName << "\n"
                    << "    eval_stats = evaluate_model(model, "
                       "loaders[\"test\"])\n"
                    << "    print(\"eval stats:\", eval_stats)\n"
                    << "\n"
                    << "    # Store selected metrics into `metrics` dict\n";
                for (const auto &metric : op.eval.metrics) {
                    out << "    metrics[\"" << metric
                        << "\"] = eval_stats.get(\"" << metric << "\", None)\n";
                }
                out << "\n";
            }
        }

        // -------------------------------------------------------------
        // 5) Wire experiment result mapping
        // -------------------------------------------------------------
        if (!e.results.empty()) {
            out << "    # Final structured result\n"
                << "    result = {}\n";
            for (const auto &rf : e.results) {
                out << "    result[\"" << rf.fieldName << "\"] = metrics.get(\""
                    << rf.metricName << "\", None)\n";
            }
            out << "    return result\n\n";
        } else {
            out << "    return {}\n\n";
        }
    }

    return out.str();
}

std::string PytorchBackend::emitMainSection(
    const PipelineModule &module,
    const std::string &selectedExperimentName) const {
    std::ostringstream out;

    out << "# ============================================================\n"
        << "# Entry point\n"
        << "# ============================================================\n"
        << "if __name__ == \"__main__\":\n";

    if (module.experiments.empty()) {
        out << "    print(\"No experiments to run.\")\n";
        return out.str();
    }

    // Choose experiment: explicit name if given and found, otherwise first.
    const IRExperimentPipeline *chosen = nullptr;

    if (!selectedExperimentName.empty()) {
        for (const auto &e : module.experiments) {
            if (e.name == selectedExperimentName) {
                chosen = &e;
                break;
            }
        }
        if (!chosen) {
            // Fall back but also make it visible in the script output.
            out << "    print(\"[warning] requested experiment '"
                << selectedExperimentName
                << "' not found; falling back to first experiment.\")\n";
            chosen = &module.experiments.front();
        }
    } else {
        chosen = &module.experiments.front();
    }

    out << "    result = run_experiment_" << chosen->name << "()\n"
        << "    print(\"Final experiment result:\")\n"
        << "    print(result)\n";

    return out.str();
}

} // namespace syntra
